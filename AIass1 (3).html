
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>AI Assignment 1</title>

</head>

<body>
	<center>
	<h1>Introduction to Artificial Intelligence </h1>
	<h3>Assignment 1</h3>
	</center>
<hr>
	<h2>Environment simulator and agents for the Hurricane Evacuation Problem</h2>
<p>
In this first exercise you are asked to implement an environment
simulator that runs a path optimization problem.
Then, you will implement some agents that live in the environment
and evaluate their performance.

</p><p>
We are given a weighted graph G=(V,E,w) and a set of target vertices S,
and the goal is (starting at a given vertex) to visit all the target vertices,
as quickly as possible. 
Unlike standard shortest path problems in graphs,
which have easy known efficient solution methods (e.g. the Dijkstra algorithm), here the
problem is that there is more than 1 vertex to visit, and their order
is not given.
This is a problem encountered in many real-world settings, such as when
you are trying to evacuate people who are stuck at home with no transportation before
the hurricane arrives.

</p><h3>Hurricane Evacuation problem environment</h3>

<p>
The environment consists of a weighted undirected graph, with weights being positive integers. Each vertex may contain
a number of people to be evacuated.
An agent (evacuation vehicle) at a vertex automatically picks up all the people at this vertex just before starting the
next move, thereby saving them. Some set B of vertices are known to be brittle, and can only be visited once and
then become blocked.
In this assignment
we assume complete knowledge so location of people to save and of brittle vertices.

</p><p>
An agent can only do  <b>traverse</b> and <b>no-op</b> actions.
The time for traverse  actions is equal to w, the edge weight (assumed to be a positive integer).
The action always succeeds if the target vertex is unblocked, otherwise is the same as <b>no-op</b> which takes one time unit.
The simulation ends when all people have been picked up, or there is no path for any agent to pick up any more people.
</p><p>
The simulator should keep track of time, the number of
actions done by each agent, and the total number of people successfully evacuated.

</p><h3>Implementation part I: simulator + simple agents</h3>
 
<p>
Initially you will implement the environment simulator, and several simple
(non-AI) agents. The environment simulator should start up by reading
the graph from a file, as well as the contents of vertices and global constants,
in a format of your choice. We suggest
using a simple adjancency list in an ASCII file, that initially specifies
the number of vertices. For example (comments beginning
with a semicolon):

</p><pre>#N 4      ; number of vertices n in graph (from 1 to n)
#V1                  ; Vertex 1, nothing of interest
#V2 P1 B             ; Vertex 2, initially contains 1 person to be rescued, and is brittle
#V3 B                ; Vertex 3, has no people and is brittle
#V4 P2               ; Vertex 4, initially contains 2 persons to be rescued

#E1 1 2 W1                 ; Edge 1 from vertex 1 to vertex 2, weight 1
#E2 3 4 W1                 ; Edge 2 from vertex 3 to vertex 4, weight 1
#E3 2 3 W1                 ; Edge 3 from vertex 2 to vertex 3, weight 1
#E4 1 3 W4                 ; Edge 4 from vertex 1 to vertex 3, weight 4
#E5 2 4 W5                 ; Edge 5 from vertex 2 to vertex 4, weight 5
</pre>

<p>
The simulator should query the user about the number of agents and
what agent program to use for each of them, from a list defined below.
Global constants and initialization parameters for each agent 
(initial position) are also to be queried from the user.

</p><p>
After the above initialization, the simulator should run each agent in turn,
performing the actions retured by the agents, and update the world
accordingly. Additionally, the simulator should be capable of displaying the
state of the world after each step, with the appropriate 
state of the agents and their score. The score of an agent is the number of people
saved by the agent times 1000, minus the time taken.
A simple screen display in ASCII is sufficient (no bonuses
for fancy graphics - this is not the point in this course!).

</p><p>
Each agent program (a function) works as follows. 
The agent is called by the simulator, together with
a set of observations. The agent returns a move to be carried out in the 
current world state. The agent is allowed to keep an internal state
(for example, a computed optimal path, or anything else desired) if needed.
In this assignment, the agents can observe the entire state of the world.

</p><p>
You should implement the following agents:

</p><ol>
<li> A <b>human</b> agent, i.e. print the state, read the next move from the user, and
return it to the simulator. This is used for debugging and evaluating the program.
</li><li> A <b> stupid greedy</b> agent, that works as follows: the agent should
compute the shortest currently unblocked path to the next vertex with people to be rescued,
and try to follow it.
If there is no such path, do <b>terminate</b>. Here and elsewhere, if needed, break ties by prefering lower-numbered
vertices and/or edges.
</li><li> A <b> saboteur</b>  agent, that breaks brittle vertices by visiting them. 
The saboteur works as follows: it computes the shortest path to a brittle vertex, and moves in that direction.
If not possible, it does a no-op. The saboteur does not pick up people.
Prefer the lowest-numbered vertices and edge in case of ties.
</li></ol>

<p>
At this stage, you should run the environment with <b>three</b> agents
participating in each run: one stupid greedy agent, one saboteur agent, and one
other agent that can be chosen by the user.
Your program should display and record the scores. In particular,
you should run the stupid greedy agent with various initial configurations.
Also, test your environment with several agents in the
same run, to see that it works correctly. You would be advised
to do a good job here w.r.t. program extensibility, modularity, etc.
much of this code may be used for some of the following
assignments as well. 

</p><p>
<b>Clarification and rationale:</b>
Note that this part of the assignment will not
really be checked, as it contains no AI, so details are not important.
The goal of this part of the assignment is constructing infrastructure 
for the rest of the assignment(s). E.g. the "human agent" is intended as
a debugging and demo aid, and also towards assignment 2, and the stupid greedy agent
contains code for shortest path that would be a component in a heuristic
in the 2nd part of this assignment.


</p><h3>Implementation part II: search agents</h3>

<p>
<b>Now</b>, after chapter 4, you will be implementing
intelligent agents (this is part 2 of the assignment)
that need to act in this environment. Each agent
should assume that it is acting alone, regardless of whether it is true.
You will be implementing a "search" agent as defined below.
All the algorithms will use an <b>admissible heuristic evaluation function</b> 
of your choice. The search algorithms you implement should 
just search for a path that minimizes the time to collect all people in the graph!

</p><ol>
<li> A greedy search agent,
 that picks the move with best immediate heuristic value to expand next.
</li><li> An agent using A* search, with the same heuristic. Assume a global constant of LIMIT
expansions (default 10000) beyond which we just return "fail", and the agent does just the "terminate"
action.
</li><li> An agent using real-time A* doing L (user determined constant,
L=10 by default) expansions before each move decision.
</li></ol>

<p>
The performance measure will be based on the "situated planning" idea of goal achievment time,
that is, recognizing that the search algorithm run also takes time!
Rather than using a real-time clock, we will simply have a per-expansion time
T, and if N expansions are run in the search
algorithm this means that N*T time has passed in real time.
The performance of an agent will thus be equal to S, when the path is actually executed in
"real time". For simplicity, we will start with T=0, that is,
assuming that search takes no time.

</p><p>
Note that if T is non-zero,
a path that looks optimal may actually
be bad because we have wasted a lot of time to find it,
when a simple quickly found path may be almost as good.
The number of expansions are: 1 per action for the greedy agent
(always 1 expansion),  and L for the "real-time A*".
We will run all algorithms with values of T being 0, 0.000001, and 0.01
and report the performance results.

</p><p>
Observe that for "situated" A* this is a very hard problem for an algorithm that considers the deadline,
as we do not know the number of
expansions before we do the search! Many search applications just assume that the number of
expansions is maximal and equal to LIMIT, or ignore the effect on the goal achievement time,
but determining how to do this in a reasonable
manner is an open research problem. (Do this to earn an MSc, or even a PhD.)

</p><p>
<b>Bonus version</b>: construct a search agent as above, but in addition
allow one saboteur agent also acting in the environment.
Your search agent needs to take this into account.
Observe that although this seems to be a multi-agent problem,
the fact that saboteur is perfectly predictable makes this in essence a single agent search problem.

</p><p>
<b>Addtional bonus - theoretical</b>: What is the computational complexity of the
Hurricane Evacuation Problem (single agent)? Can you prove that it is NP-hard? Or is it
in P? If the latter, can you design an algorithm that solves the problem in polynomial time?

</p><h2>Deliverables</h2>

<p>
The program and code sent to the grader, by e-mail or
otherwise as specified by the grader, a printout of the code and results.
A document stating the heuristic function you used and the rationale
for selecting this function.
Set up a time for frontal grading checking of the delivered assignment,
in which both members of each team must demonstrate at least <b>some</b>
familiarity with their program...

</p><p>

Due date for part 1 (recommended, not checked or enforced): Tuesday, November 15, 2022.
</p><p>
For the complete assignment: Tuesday, November 29.


</p></body></html>